# banking-machine-learning
The purpose of this case study is to use machine learning to maximize revenue from marketing campaigns in the banking sector.
Made in collaboration with E. Galin and R. Cheng @emlyon business school


## STEP 1: FRAME THE PROBLEM AND DEFINE THE OBJECTIVES

### Project objective #1:
Identify 100 clients who are most likely to accept one of the marketing offers from the bank:
• MF (mutual fund)
• CC (credit card)
• CL (customer loan)

### Project Objective #2:
Predict and calculate the expected revenue from those customers. Important: each client can be targeted only once.
Our prediction model should answer the following questions:
• Who are the clients more likely to accept a consumer loan offer?
• Who are the clients more likely to accept a credit card offer?
• Who are the clients more likely to accept a mutual fund offer?
• Which are the 100 top clients to target with bank offers? What offers (MF, CC or CL)? • What is the expected revenue from these 100 clients for each type of offer?
The data includes 3 types of marketing offers and the revenue generated by these offers. This makes it a supervised classification and regression machine learning task:
• Supervised: we have access to the features and the target and our aim is to train a model that can learn a mapping between the two.
• Classification: we need to predict which type of offer a customer will accept (MF, CC or CL).
• Regression: revenue is a continuous variable.
Since there are two types of problems, each task will be carried out separately. However,
preparation steps are the same for both problems.

## STEP 2: EXPLORE THE DATA
After joining the 4 datasets together and carefully analysing the outcome, we found out that 60% of the clients have already received one of the following back offers:

• Mutual Fund (MF) - 193 clients out of 969
• Credit Card (CC) – 242 clients out of 969
• Consumer Loan (CL) - 290 clients out of 969
Thus, we decided to build a prediction model based on this information and use it to identify 100 potential clients from the remaining 40%.
As a result, our dependent variables (targets) are the following: Sale_MF, Sal_CC, Sale_CL, Revenue_MF, Revenue_CC, Revenue_CL, while all the rest are our features (independent variables).
We tried to get a better understanding of the dataset after uploading it into Python. We used df.shape, df.columns, and df.head() to see what features we work with and what each feature entails. Our next step is to check datatypes, missing values, think of how to deal with them, transform some variables and remove outliers if any.

## STEP 3: DATA PREPARATION

### 3.1. MISSING VALUES
Quite a lot of missing values were identified within the data frame. Some variables contained more than 70% of missing values.
Though these values have occurred due to the specificity of the relationships with the bank (e.g. customers have no bank product or no transactions happen in their account), we decided to delete the columns with more than 70% missing values.
Missing values in categorical column ‘Sex’ were replaced with the most common value. Other missing variables were replaced with zero values as they identify that a customer does not use one of the banking products.
 
### 3.2. CATEGORICAL FEATURES
It is important to deal with categorical features as some ML models cannot deal with them. In our case categorical variable ‘Sex’ was replaced with dummy variables for each gender. Dummy variable encoding was chosen as it treats data equally without prioritizing one of the categories.

### 3.3. DROPPED ROWS
Customers younger than 18 are not interested in any of the products. They have only saving accounts which were created by their parents in order to be used for future needs (education etc.). Thus, we decided to delete these customers from the dataset.

### 3.4. DATA NORMALISATION & SCALING
After visualizing the data with the help of histograms, we noticed that data is highly skewed and contains a lot of outliers. Many algorithms are sensitive towards outliers and lack of symmetry. Moreover, values lie in different ranges, thus it is important to scale these values for future modelling. Scaling modifies features and improves stability of some models.
Each numerical variable in the dataset (except target variables) was treated in order to normalize and scale it. Box Cox transformation was used to get rid of skewness, while min- max scaling was used to scale the data between 0 and 1.
 
### 3.5. FEATURE SELECTION
It is important to choose only those independent variables which are not correlated between each other as positive correlation between independent variables implies a linear relationship between them. After checking the correlation, some variables were selected to be deleted as they were highly correlated.
 
## STEP 4: RUNNING MODELS. CLASSIFICATION 

### 4.1. SPLITTING THE DATA
Next step was to assign values to X and y and split our data into training set (80%) and testing set (20%) with the help of train-test_split.

### 4.2. RUN FIRST DIRTY MODELS
Next step was to run first ‘dirty’ models using standard parameters and measure their performance. For each model, N-fold cross validation method was used and accuracy, recall, precision and confusion matrices were calculated.
After the first model run, it was noticed that the data is highly imbalanced: there are more people who rejected the offer (y =0), than those who accepted it. As a result, our model tends to predict mostly y=0 outcome.

### 4.3. APPLYING SMOTE
In order to tackle this problem, over-sampling technique for class-imbalanced data (SMOTE) was applied. As a result, we achieved equal negative and positive classes. Application of this technique helped to significantly improve our results (recall & precision).
 
### AFTER SMOTE:
Since we have a limited number of customers to contact, precision is a more important metric for us. As a result, we choose the models that have a higher precision score.
The following models prove to show high precision score:
• Random Forests (77%)
• K Nearest Neighbors (66%)
• Decision Trees (69%)
• Gradient Boost Classifier (68%)
Thus, the four models above were chosen for further fine-tuning.

### 4.4. FINE-TUNING THE MODELS
With the help of Grid Search the best hyperparameters for each model were identified. They
helped to improve the performance of the models (precision).
Random Forests (77% to 80%)
K Nearest Neighbors (66% to 75%) Decision Trees (69% to 71%)
Gradient Boost Classifier (68% to 74%)
Since Random Forests Model showed the best results, it was chosen as our final model.
 
### 4.5. CHECK MODEL PERFORMANCE ON TEST SET
After applying the chosen model to test set, we achieved the following results:
The precision of the model dropped significantly. Perhaps, if we had more time to explore correlations between variables, we would be able to overcome the problem of overfitting. However, for the moment it is the best we could obtain at this stage of the project.

## STEP 5: APPLY THE MODEL TO PREDICT CUSTOMERS
In order to apply the chosen model to predict customers who are most likely to accept one of the offers, we first need to scale and normalize features in the data set used for prediction. We apply the same techniques as for the variables in the train data set.
  
Next, we apply the Random Forest model to predict those customers who are most likely to accept an offer as well as calculate the probability score of the event to happen.
Out of 606 customers our models predicted that 117 are likely to accept the offer. Below you can see results for Mutual Fund (with probability scores for each identified customer). Full results you can find in the document Revenue_MF.
The results are saved in a document and will be used for future manipulations.
The same procedure is performed separately for each of the three offers (Mutual fund, Customer loan and Credit card): 125 customers identified as most likely to accept Credit card offer and 143 customers as those who might accept Customer loan offer.
Now, when we know which customers are most likely to accept one of our offers, we can predict and calculate revenue we could extract from those customers.
 
### REGRESSION
In order to do this, the same data preparation steps have been carried out (steps 2 to 3). The only manipulation that is different is removing those customers, who did not accept the offer, from the training dataset.
For example, while preparing the training dataset to predict revenue from the mutual fund offer, we removed all the customers, who did not accept the offer and for which Revenue_MF is equal to zero. We did it deliberately since the model will be applied only to those customers who have high probability of accepting offer. As a result, we trained our model on customers who have already accepted the offer.

### RUNNING FIRST MODELS
Since the nature of the problem is different (regression problem), different models were used. However, in order to identify the best model, we applied the same steps as previously for the classification problem (running a dirty model, tuning hyperparameters, calculating accuracy).
Out of four models, the one, that showed the best performance was the Random Forest Regression model.
Further, data for prediction was prepared in the same way as for the classification problem (normalized and scaled).
Finally, the predicted revenue for each offer and each potential client was calculated.
 
   The same procedure was performed separately for each banking offer. The full results for all the identified customers could be found in documents attached with the report.

## STEP 5. PRESENTING FINAL RESULTS
As a final step, all the three tables with results have been joined together and sorted according to the probability score (descending order). Since each customer could be contacted only once, duplicated clients were dropped (only the first occurrence was kept).
As a result, we constructed a table with 100 customers to contact with different offers, their probability score and predicted revenue.

Total predicted revenue for each offer was calculated with the help of pivot_table:
Out of 100 customers we identified 31 clients to contact with Credit card offer, 41 – with Customer loan and 28 – with Mutual fund. We predict to get a total revenue of 274.44 from Credit card customers, 535.80 – from Customer loan customers and 203.94 – from Mutual fund customers. The total predicted revenue is 1014.18.
